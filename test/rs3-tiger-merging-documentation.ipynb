{
 "metadata": {
  "name": "",
  "signature": "sha256:7766bceed6c2b1e4558278937110a57929383886848e18e8d592e2e662212dfc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import os\n",
      "import re\n",
      "import codecs\n",
      "from lxml import etree\n",
      "import networkx\n",
      "\n",
      "from discoursegraphs.io.rst import RSTGraph\n",
      "from discoursegraphs.io.tiger import TigerDocumentGraph\n",
      "\n",
      "DOC_ID_REGEX = '\\d+'\n",
      "\n",
      "TIGER_DIR = os.path.expanduser(\"~/repos/pcc-annis-merged/maz176/syntax\")\n",
      "RS3_DIR = os.path.expanduser(\"~/repos/pcc-annis-merged/maz176/rst\")\n",
      "TOKENIZED_DIR = os.path.expanduser(\"~/repos/pcc-annis-merged/maz176/tokenized\")\n",
      "\n",
      "TIGER_FILEPATHS = !ls $TIGER_DIR/syntax.*.xml\n",
      "RS3_FILEPATHS = !ls $RS3_DIR/*.rs3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_rst_tokenlist(rst_graph):\n",
      "    \"\"\"\n",
      "    extracts all tokens from an RSTGraph.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    rst_graph : RSTGraph\n",
      "        a directed graph representing an RST tree\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    all_rst_tokens : tuple of (unicode, str)\n",
      "        a list of (str, str) tuples, where the first element is the token\n",
      "        and the second one is the segment node ID it belongs to.\n",
      "    \"\"\"\n",
      "    all_rst_tokens = []\n",
      "    for segment_id in rstgraph.segments:\n",
      "        segment_tokens = [(token, segment_id) \n",
      "                              for token in rstgraph.node[segment_id]['rst:text'].split()]\n",
      "        all_rst_tokens.extend(segment_tokens)\n",
      "    return all_rst_tokens\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_tiger_tokenlist(tigerdoc_graph):\n",
      "    \"\"\"\n",
      "    extracts all tokens from a TigerDocumentGraph.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tigerdoc_graph : TigerDocumentGraph\n",
      "        a directed graph representing a TigerXML file and all the\n",
      "        annotated sentences found in it.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    all_tiger_tokens : tuple of (unicode, str, str)\n",
      "        a list of (unicode, str, str) tuples, where the first element\n",
      "        is the token, the second is the sentence root node ID (of the)\n",
      "        corresponding sentence and the third is the token node ID.\n",
      "    \"\"\"\n",
      "    all_tiger_tokens = []\n",
      "    for sent_id in tigerdocgraph.sentences:\n",
      "        tiger_sent_tokens = [(tigerdocgraph.node[token_id]['tiger:word'], sent_id, token_id)\n",
      "                                 for token_id in tigerdocgraph.node[sent_id]['tokens']]\n",
      "        all_tiger_tokens.extend(tiger_sent_tokens)\n",
      "    return all_tiger_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_doc_id(annotation_file_path):\n",
      "    \"\"\"\n",
      "    extracts the numerical document ID from the file name of an annotation file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    annotation_file_path : str\n",
      "        absolute or relative path to an annotation file\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    doc_id : str\n",
      "        the numerical document ID of the annotation file, e.g. '00001' or '14881'\n",
      "    \"\"\"\n",
      "    file_name = os.path.basename(annotation_file_path)\n",
      "    return re.search(DOC_ID_REGEX, file_name).group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for file_count, tiger_path in enumerate(TIGER_FILEPATHS):\n",
      "    rs3_path = RS3_FILEPATHS[file_count]\n",
      "    if get_doc_id(tiger_path) == get_doc_id(rs3_path):\n",
      "        tigerdocgraph = TigerDocumentGraph(tiger_path)\n",
      "        all_tiger_tokens = gen_tiger_tokenlist(tigerdocgraph)\n",
      "        \n",
      "        rstgraph = RSTGraph(rs3_path)\n",
      "        all_rst_tokens = gen_rst_tokenlist(rstgraph) # list of (token str, segment node ID str)\n",
      "    else:\n",
      "        raise ValueError(\"Can't match file names:\\n{0}\\n{1}\".format(tiger_path, rs3_path))\n",
      "        \n",
      "    tigerdocgraph.add_nodes_from(rstgraph.nodes(data=True))\n",
      "    tigerdocgraph.add_edges_from(rstgraph.edges(data=True))\n",
      "    for i, (tiger_token, tiger_sent_id, tiger_token_id) in enumerate(all_tiger_tokens):\n",
      "        try:\n",
      "            rst_token, rst_segment_node_id = all_rst_tokens[i]\n",
      "            if tiger_token == all_rst_tokens[i][0]:\n",
      "                tigerdocgraph.add_node(tiger_token_id, layers={'rst', 'rst:token'},\n",
      "                                       attr_dict={'rst:token': rst_token})\n",
      "                tigerdocgraph.add_edge(int(rst_segment_node_id), tiger_token_id,\n",
      "                                       layers={'rst', 'rst:token'})\n",
      "            else: # token mismatch\n",
      "                tokenized_path = os.path.join(TOKENIZED_DIR, rs3_path.split('.')[0]+'.tok.txt')\n",
      "                print(\"geany {0} {1} {2}\\n\"\n",
      "                      \"{3} != {4}\".format(os.path.join(RS3_DIR, rs3_path), tiger_path, tokenized_path,\n",
      "                                                        all_tiger_tokens[i], all_rst_tokens[i]))\n",
      "        except IndexError:\n",
      "            print(\"IndexError in all_rst_tokens[{0}][0]. List has only {1} elements.\".format(i, len(all_rst_tokens)))\n",
      "print(\"finished aligment.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished aligment.\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tigerdocgraph.nodes(data=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Looks fine, move on to Conano and MMAX\n",
      "\n",
      "```\n",
      "('s2155_4',\n",
      "  {'layers': {'rst', 'rst:token', 'tiger', 'tiger:token'},\n",
      "   'rst:token': u'gut',\n",
      "   'tiger:id': 's2155_4',\n",
      "   'tiger:lemma': '--',\n",
      "   'tiger:morph': '--',\n",
      "   'tiger:pos': 'ADJD',\n",
      "   'tiger:word': u'gut'}),\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}