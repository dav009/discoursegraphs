{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import os\n",
      "import codecs\n",
      "from lxml import etree\n",
      "import networkx\n",
      "\n",
      "import rst2networkx\n",
      "import tigerxml2networkx\n",
      "\n",
      "TIGER_DIR = \"/home/arne/repos/pcc-annis-merged/maz176/syntax/\"\n",
      "RS3_DIR = \"/home/arne/repos/pcc-annis-merged/maz176/rst/\"\n",
      "TOKENIZED_DIR = \"/home/arne/repos/pcc-annis-merged/maz176/tokenized/\"\n",
      "\n",
      "TIGER_TESTFILE_PATHS = !ls /home/arne/repos/pcc-annis-merged/maz176/syntax/syntax.*.xml\n",
      "RS3_TESTFILE_PATHS = !ls *.rs3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_rst_tokenlist(rst_graph):\n",
      "    \"\"\"\n",
      "    extracts all tokens from an RSTGraph.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    rst_graph : RSTGraph\n",
      "        a directed graph representing an RST tree\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    all_rst_tokens : tuple of (unicode, str)\n",
      "        a list of (str, str) tuples, where the first element is the token\n",
      "        and the second one is the segment node ID it belongs to.\n",
      "    \"\"\"\n",
      "    all_rst_tokens = []\n",
      "    for segment_id in rstgraph.segments:\n",
      "        segment_tokens = [(token, segment_id) \n",
      "                              for token in rstgraph.node[segment_id]['text'].split()]\n",
      "        all_rst_tokens.extend(segment_tokens)\n",
      "    return all_rst_tokens\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_tiger_tokenlist(tigerdoc_graph):\n",
      "    \"\"\"\n",
      "    extracts all tokens from a TigerDocumentGraph.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tigerdoc_graph : TigerDocumentGraph\n",
      "        a directed graph representing a TigerXML file and all the\n",
      "        annotated sentences found in it.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    all_tiger_tokens : tuple of (unicode, str, str)\n",
      "        a list of (unicode, str, str) tuples, where the first element\n",
      "        is the token, the second is the sentence root node ID (of the)\n",
      "        corresponding sentence and the third is the token node ID.\n",
      "    \"\"\"\n",
      "    all_tiger_tokens = []\n",
      "    for sent_id in tigerdocgraph.sentences:\n",
      "        tiger_sent_tokens = [(tigerdocgraph.node[token_id]['word'], sent_id, token_id)\n",
      "                                 for token_id in tigerdocgraph.node[sent_id]['tokens']]\n",
      "        all_tiger_tokens.extend(tiger_sent_tokens)\n",
      "    return all_tiger_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for file_count, tiger_path in enumerate(TIGER_TESTFILE_PATHS):\n",
      "    rs3_path = RS3_TESTFILE_PATHS[file_count]\n",
      "    if os.path.basename(tiger_path.split('.')[1]) == rs3_path.split('.')[0]:\n",
      "        tigerdocgraph = tigerxml2networkx.TigerDocumentGraph(tiger_path)\n",
      "        all_tiger_tokens = gen_tiger_tokenlist(tigerdocgraph)\n",
      "        \n",
      "        rstgraph = rst2networkx.RSTGraph(rs3_path)\n",
      "        all_rst_tokens = gen_rst_tokenlist(rstgraph)\n",
      "        \n",
      "    for i, (tiger_token, sent_id, token_id) in enumerate(all_tiger_tokens):\n",
      "        try:\n",
      "            if not tiger_token == all_rst_tokens[i][0]:\n",
      "                tokenized_path = os.path.join(TOKENIZED_DIR, rs3_path.split('.')[0]+'.tok.txt')\n",
      "                print(\"geany {0} {1} {2}\\n\"\n",
      "                      \"{3} != {4}\".format(os.path.join(RS3_DIR, rs3_path), tiger_path, tokenized_path,\n",
      "                                                        all_tiger_tokens[i], all_rst_tokens[i]))\n",
      "        except IndexError:\n",
      "            print(\"IndexError in all_rst_tokens[{0}][0]. List has only {1} elements.\".format(i, len(all_rst_tokens)))\n",
      "print(\"finished aligment.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished aligment.\n"
       ]
      }
     ],
     "prompt_number": 49
    }
   ],
   "metadata": {}
  }
 ]
}