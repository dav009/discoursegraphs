{
 "metadata": {
  "name": "",
  "signature": "sha256:54291ab0b0f68b8e25b9230116410300d955798fe3fb1f23d661f43f43430427"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import networkx\n",
      "from itertools import chain\n",
      "\n",
      "from discoursegraphs.util import ensure_unicode\n",
      "from discoursegraphs.io.tiger import TigerDocumentGraph\n",
      "from discoursegraphs.io.anaphoricity import AnaphoraDocumentGraph\n",
      "\n",
      "DOC_ID_REGEX = '\\d+'\n",
      "\n",
      "PRIMARY_TEXT_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/primary-data')\n",
      "TOKENIZED_TEXT_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/tokenized')\n",
      "TIGER_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/syntax')\n",
      "DAS_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/das')\n",
      "ES_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/es')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tiger_files = !ls $TIGER_DIR/*.xml\n",
      "das_files = !ls $DAS_DIR/*.txt\n",
      "es_files = !ls $ES_DIR/*.txt\n",
      "\n",
      "def get_doc_id(annotation_file_path):\n",
      "    \"\"\"\n",
      "    extracts the numerical document ID from the file name of an annotation file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    annotation_file_path : str\n",
      "        absolute or relative path to an annotation file\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    doc_id : str\n",
      "        the numerical document ID of the annotation file, e.g. '00001' or '14881'\n",
      "    \"\"\"\n",
      "    file_name = os.path.basename(annotation_file_path)\n",
      "    return re.search(DOC_ID_REGEX, file_name).group()\n",
      "\n",
      "def doc_id2tiger_filename(doc_id, tiger_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a Tiger file path (as it is used in PCC).\n",
      "    \"\"\"\n",
      "    return os.path.join(tiger_dir, 'syntax.maz-{}.xml'.format(doc_id))\n",
      "\n",
      "def doc_id2primarytext_filename(doc_id, primary_text_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a primary text file path (as it is used in PCC).    \n",
      "    \"\"\"\n",
      "    return os.path.join(primary_text_dir, 'maz-{}.txt'.format(doc_id))\n",
      "                        \n",
      "def doc_id2tokenization_filename(doc_id, tokenized_text_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a tokenized text file path (as it is used in PCC).    \n",
      "    \"\"\"\n",
      "    return os.path.join(tokenized_text_dir, 'maz-{}.tok.txt'.format(doc_id))\n",
      "\n",
      "def doc_id2anaphoricity_filename(doc_id, anaphoricity_dir, annotated_word):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into an anaphora annotation file path (as it is used in PCC).\n",
      "    \"\"\"\n",
      "    assert annotated_word in ('das', 'es')\n",
      "    return os.path.join(anaphoricity_dir, 'maz-{0}.anaphora.{1}.txt'.format(doc_id, annotated_word))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each anaphoricity file, there's one Tiger file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all([os.path.isfile(doc_id2tiger_filename(doc_id, TIGER_DIR)) \n",
      "         for doc_id in (get_doc_id(fpath)\n",
      "            for fpath in tiger_files)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANAPHORICITY_TEST_FILE = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-10207.anaphora.das.txt')\n",
      "TIGER_TEST_FILE = doc_id2tiger_filename(get_doc_id(ANAPHORICITY_TEST_FILE), TIGER_DIR)\n",
      "\n",
      "adg = AnaphoraDocumentGraph(ANAPHORICITY_TEST_FILE)\n",
      "tdg = TigerDocumentGraph(TIGER_TEST_FILE)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# We need to ensure unicode in anaphoricity.py\n",
      "\n",
      "# we need to match the TigerXML tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_anaphoricity_token(anaphora_docgraph, anaphoricity_token_id):\n",
      "    \"\"\"\n",
      "    returns the unicode token string of an 'anaphoricity' token\n",
      "    \"\"\"\n",
      "    from discoursegraphs.util import ensure_unicode\n",
      "    return ensure_unicode(anaphora_docgraph.node[anaphoricity_token_id]['token'])\n",
      "\n",
      "def get_tiger_token_ids(tiger_docgraph):\n",
      "    \"\"\"returns a list of token nodes from an TigerDocumentGraph\"\"\"\n",
      "    from itertools import chain\n",
      "    return list(chain.from_iterable((tiger_docgraph.node[root_id]['tokens'] \n",
      "                                     for root_id in tiger_docgraph.sentences)))\n",
      "\n",
      "def get_tiger_token(tiger_docgraph, tiger_token_id):\n",
      "    \"\"\"\n",
      "    returns the unicode token string of a tiger token    \n",
      "    \"\"\"\n",
      "    return tiger_docgraph.node[tiger_token_id]['word']\n",
      "\n",
      "error_count = 0\n",
      "for annotation_dir in (das_files, es_files):\n",
      "    for annotation_file in annotation_dir:\n",
      "        adg = AnaphoraDocumentGraph(annotation_file)\n",
      "        tiger_file = doc_id2tiger_filename(get_doc_id(annotation_file), TIGER_DIR)\n",
      "        tdg = TigerDocumentGraph(tiger_file)\n",
      "\n",
      "        adg_token_ids = adg.tokens\n",
      "        tdg_token_ids = get_tiger_token_ids(tdg)\n",
      "\n",
      "        for i, node_id in enumerate(adg_token_ids):\n",
      "            anaphora_token = get_anaphoricity_token(adg, node_id)\n",
      "            try:\n",
      "                tiger_token = get_tiger_token(tdg, tdg_token_ids[i])\n",
      "            except IndexError as e:\n",
      "                print annotation_file\n",
      "                print \"tdg_token_ids has no element w/ number {0}\\n\".format(i)\n",
      "\n",
      "\n",
      "            if anaphora_token != tiger_token:\n",
      "                error_count += 1\n",
      "                print \" geany\", annotation_file, doc_id2anaphoricity_filename(get_doc_id(annotation_file), DAS_DIR, 'das'), \\\n",
      "                    tiger_file, \\\n",
      "                    doc_id2primarytext_filename(get_doc_id(annotation_file), PRIMARY_TEXT_DIR), \\\n",
      "                    doc_id2tokenization_filename(get_doc_id(annotation_file), TOKENIZED_TEXT_DIR)\n",
      "                print u\"tokens don't match: {0} (anaphoricity) vs. {1} (tiger)\".format(anaphora_token, tiger_token)\n",
      "                try:\n",
      "#                     print \"index: {0}, node_id: {1}\".format(i, node_id)\n",
      "                    print u\"\\tanaphoricity context: {0} {1} __{2}__ {3}\".format(get_anaphoricity_token(adg, adg_token_ids[i-2]), \n",
      "                                                               get_anaphoricity_token(adg, adg_token_ids[i-1]),\n",
      "                                                               get_anaphoricity_token(adg, adg_token_ids[i]),\n",
      "                                                               get_anaphoricity_token(adg, adg_token_ids[i+1]))\n",
      "                    print u\"\\ttiger context: {0} {1} __{2}__ {3}\\n\".format(get_tiger_token(tdg, tdg_token_ids[i-2]),\n",
      "                                                                       get_tiger_token(tdg, tdg_token_ids[i-1]),\n",
      "                                                                       get_tiger_token(tdg, tdg_token_ids[i]),\n",
      "                                                                       get_tiger_token(tdg, tdg_token_ids[i+1]))\n",
      "                except:\n",
      "                    print \"\\n\"\n",
      "                break\n",
      "print \"finished alignment check. {} errors.\".format(error_count)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finished alignment check. 0 errors.\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the tokenizations match. Now, lets merge the anaphora annotation into the Tiger document graph:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "ANAPHORICITY_TEST_FILE = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-10207.anaphora.das.txt')\n",
      "TIGER_TEST_FILE = doc_id2tiger_filename(get_doc_id(ANAPHORICITY_TEST_FILE), TIGER_DIR)\n",
      "\n",
      "adg = AnaphoraDocumentGraph(ANAPHORICITY_TEST_FILE)\n",
      "tdg = TigerDocumentGraph(TIGER_TEST_FILE)\n",
      "tdg_token_ids = get_tiger_token_ids(tdg)\n",
      "\n",
      "anaphora2tiger = {} # maps from anaphoricity token node IDs to tiger token node IDs\n",
      "for i, anaphora_node_id in enumerate(adg.tokens):\n",
      "    anaphora_token = get_anaphoricity_token(adg, anaphora_node_id)\n",
      "    tiger_token_id = tdg_token_ids[i]\n",
      "    tiger_token = get_tiger_token(tdg, tiger_token_id)\n",
      "    if anaphora_token == tiger_token:\n",
      "        anaphora2tiger[anaphora_node_id] = tiger_token_id \n",
      "    else:\n",
      "        raise ValueError(u\"tokens don't match: {0} (anaphoricity) vs. {1} (tiger)\".format(anaphora_token, tiger_token))\n",
      "\n",
      "networkx.relabel_nodes(adg, anaphora2tiger, copy=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "<discoursegraphs.io.anaphoricity.AnaphoraDocumentGraph at 0x338b9d0>"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tdg.add_nodes_from(adg.nodes(data=True))\n",
      "tdg.add_edges_from(adg.edges(data=True))\n",
      "\n",
      "# tdg.nodes(data=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TODOs\n",
      "\n",
      "## anaphoricity.py\n",
      "\n",
      "* annotation --> anaphoricity:annotation {a:value, a:certainty}\n",
      "\n",
      "## tiger.py\n",
      "\n",
      "* add layer node/edge attrib\n",
      "* add layer info to annotations, e.g. tiger:secedge instead of secedge\n",
      "\n",
      "## new base class\n",
      "\n",
      "* add_nodes_from/edges_from shall not overwrite node/edge attribute if its called 'layers'\n",
      "* instead, they should be joined, i.e. layers={'tiger', 'tiger:token', 'anaphoricity', 'anaphoricity:token'}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}