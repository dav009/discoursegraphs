{
 "metadata": {
  "name": "",
  "signature": "sha256:949177dad6a344b74e77230ef120eb4ca33c642d5e4c0d7e3b36c8e629914875"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "from itertools import chain\n",
      "\n",
      "from discoursegraphs.util import ensure_unicode\n",
      "from discoursegraphs.io.tiger import TigerDocumentGraph\n",
      "from discoursegraphs.io.anaphoricity import AnaphoraDocumentGraph\n",
      "\n",
      "DOC_ID_REGEX = '\\d+'\n",
      "\n",
      "PRIMARY_TEXT_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/primary-data')\n",
      "TOKENIZED_TEXT_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/tokenized')\n",
      "TIGER_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/syntax')\n",
      "DAS_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/das')\n",
      "ES_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/es')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tiger_files = !ls $TIGER_DIR/*.xml\n",
      "das_files = !ls $DAS_DIR/*.txt\n",
      "es_files = !ls $ES_DIR/*.txt\n",
      "\n",
      "def get_doc_id(annotation_file_path):\n",
      "    \"\"\"\n",
      "    extracts the numerical document ID from the file name of an annotation file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    annotation_file_path : str\n",
      "        absolute or relative path to an annotation file\n",
      "        \n",
      "    Returns\n",
      "    -------\n",
      "    doc_id : str\n",
      "        the numerical document ID of the annotation file, e.g. '00001' or '14881'\n",
      "    \"\"\"\n",
      "    file_name = os.path.basename(annotation_file_path)\n",
      "    return re.search(DOC_ID_REGEX, file_name).group()\n",
      "\n",
      "def doc_id2tiger_filename(doc_id, tiger_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a Tiger file path (as it is used in PCC).\n",
      "    \"\"\"\n",
      "    return os.path.join(tiger_dir, 'syntax.maz-{}.xml'.format(doc_id))\n",
      "\n",
      "def doc_id2primarytext_filename(doc_id, primary_text_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a primary text file path (as it is used in PCC).    \n",
      "    \"\"\"\n",
      "    return os.path.join(primary_text_dir, 'maz-{}.txt'.format(doc_id))\n",
      "                        \n",
      "def doc_id2tokenization_filename(doc_id, tokenized_text_dir):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into a tokenized text file path (as it is used in PCC).    \n",
      "    \"\"\"\n",
      "    return os.path.join(tokenized_text_dir, 'maz-{}.tok.txt'.format(doc_id))\n",
      "\n",
      "def doc_id2anaphoricity_filename(doc_id, anaphoricity_dir, annotated_word):\n",
      "    \"\"\"\n",
      "    converts a numerical document ID into an anaphora annotation file path (as it is used in PCC).\n",
      "    \"\"\"\n",
      "    assert annotated_word in ('das', 'es')\n",
      "    return os.path.join(anaphoricity_dir, 'maz-{0}.anaphora.{1}.txt'.format(doc_id, annotated_word))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each anaphoricity file, there's one Tiger file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all([os.path.isfile(doc_id2tiger_filename(doc_id, TIGER_DIR)) \n",
      "         for doc_id in (get_doc_id(fpath)\n",
      "            for fpath in tiger_files)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ANAPHORICITY_TEST_FILE = os.path.expanduser('~/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-10207.anaphora.das.txt')\n",
      "TIGER_TEST_FILE = doc_id2tiger_filename(get_doc_id(ANAPHORICITY_TEST_FILE), TIGER_DIR)\n",
      "\n",
      "adg = AnaphoraDocumentGraph(ANAPHORICITY_TEST_FILE)\n",
      "tdg = TigerDocumentGraph(TIGER_TEST_FILE)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# We need to ensure unicode in anaphoricity.py\n",
      "\n",
      "# we need to match the TigerXML tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_anaphoricity_token_ids(anaphora_docgraph):\n",
      "    \"\"\"returns a list of token nodes from an AnaphoraDocumentGraph\"\"\"\n",
      "    token_ids = []\n",
      "    for node_id in anaphora_docgraph.nodes_iter():\n",
      "        if 'anaphoricity:token' in anaphora_docgraph.node[node_id]['layers']:\n",
      "            token_ids.append(node_id)\n",
      "    return token_ids\n",
      "\n",
      "def get_anaphoricity_token(anaphora_docgraph, anaphoricity_token_id):\n",
      "    \"\"\"\n",
      "    returns the unicode token string of an 'anaphoricity' token\n",
      "    \"\"\"\n",
      "    from discoursegraphs.util import ensure_unicode\n",
      "    return ensure_unicode(anaphora_docgraph.node[anaphoricity_token_id]['token'])\n",
      "\n",
      "def get_tiger_token_ids(tiger_docgraph):\n",
      "    \"\"\"returns a list of token nodes from an TigerDocumentGraph\"\"\"\n",
      "    from itertools import chain\n",
      "    return list(chain.from_iterable((tiger_docgraph.node[root_id]['tokens'] \n",
      "                                     for root_id in tiger_docgraph.sentences)))\n",
      "\n",
      "def get_tiger_token(tiger_docgraph, tiger_token_id):\n",
      "    \"\"\"\n",
      "    returns the unicode token string of a tiger token    \n",
      "    \"\"\"\n",
      "    return tiger_docgraph.node[tiger_token_id]['word']\n",
      "\n",
      "\n",
      "for das_file in das_files:\n",
      "    adg = AnaphoraDocumentGraph(das_file)\n",
      "    tiger_file = doc_id2tiger_filename(get_doc_id(das_file), TIGER_DIR)\n",
      "    tdg = TigerDocumentGraph(tiger_file)\n",
      "\n",
      "    adg_token_ids = get_anaphoricity_token_ids(adg)\n",
      "    tdg_token_ids = get_tiger_token_ids(tdg)\n",
      "  \n",
      "#     if len(adg_token_ids) != len(adg_token_ids):\n",
      "#         print das_file\n",
      "#         print len(adg_token_ids), len(adg_token_ids)\n",
      "\n",
      "    for i, node_id in enumerate(adg_token_ids):\n",
      "        anaphora_token = get_anaphoricity_token(adg, node_id)\n",
      "        try:\n",
      "            tiger_token = get_tiger_token(tdg, tdg_token_ids[i])\n",
      "        except IndexError as e:\n",
      "            print das_file\n",
      "            print \"tdg_token_ids has no element w/ number {0}\\n\".format(i)\n",
      "#             pass\n",
      "            \n",
      "        if anaphora_token != tiger_token:\n",
      "            print \" geany\", das_file, doc_id2anaphoricity_filename(get_doc_id(das_file), ES_DIR, 'es'), \\\n",
      "                tiger_file, \\\n",
      "                doc_id2primarytext_filename(get_doc_id(das_file), PRIMARY_TEXT_DIR), \\\n",
      "                doc_id2tokenization_filename(get_doc_id(das_file), TOKENIZED_TEXT_DIR)\n",
      "            print u\"tokens don't match: {0} (anaphoricity) vs. {1} (tiger)\".format(anaphora_token, tiger_token)\n",
      "            try:\n",
      "                print \"index: {0}, node_id: {1}\".format(i, node_id)\n",
      "                print u\"\\tanaphoricity context: {0} {1} __{2}__ {3}\\n\".format(get_anaphoricity_token(adg, adg_token_ids[i-2]), \n",
      "                                                           get_anaphoricity_token(adg, adg_token_ids[i-1]),\n",
      "                                                           get_anaphoricity_token(adg, adg_token_ids[i]),\n",
      "                                                           get_anaphoricity_token(adg, adg_token_ids[i+1]))\n",
      "                print u\"\\ttiger context: {0} {1} __{2}__ {3}\\n\".format(get_tiger_token(tdg, tdg_token_ids[i-2]),\n",
      "                                                                   get_tiger_token(tdg, tdg_token_ids[i-1]),\n",
      "                                                                   get_tiger_token(tdg, tdg_token_ids[i]),\n",
      "                                                                   get_tiger_token(tdg, tdg_token_ids[i+1]))\n",
      "            except:\n",
      "                print \"\\n\"\n",
      "            break\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " geany /home/arne/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-3415.anaphora.das.txt /home/arne/repos/pcc-annis-merged/maz176/anaphora/tosik/es/maz-3415.anaphora.es.txt /home/arne/repos/pcc-annis-merged/maz176/syntax/syntax.maz-3415.xml /home/arne/repos/pcc-annis-merged/maz176/primary-data/maz-3415.txt /home/arne/repos/pcc-annis-merged/maz176/tokenized/maz-3415.tok.txt\n",
        "tokens don't match: und (anaphoricity) vs. Austausch (tiger)\n",
        "index: 248, node_id: 249\n",
        "\tanaphoricity context: zwischen wirtschaftlichem __und__ damit\n",
        "\n",
        "\ttiger context: zwischen wirtschaftlichem __Austausch__ und\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fail_adg = AnaphoraDocumentGraph('/home/arne/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-3415.anaphora.das.txt')\n",
      "for node in fail_adg.nodes():\n",
      "    print fail_adg.node[node]['token'],"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Der aufgezwungene Krieg Der Krieg , der jetzt in Afghanistan ausgefochten wird , hat nicht erst am Sonntag Abend begonnen . Er begann am 11. September mit den Anschl\u00e4gen von New York und Washington , die in ihrer Wirkung einem geballten Raketenangriff durchaus vergleichbar sind . Wenn sich hierzulande dennoch vielerorts Zweifel an Sinn und Richtigkeit der Milit\u00e4rschl\u00e4ge breitmachen , liegt das nicht zuletzt daran , dass Freiheit und Toleranz zu den tief verwurzelten Grundwerten unserer Gesellschaft geh\u00f6ren : Wir sind es gewohnt , dass durch friedliche Verhandlungen und Kompromiss-Suche Konflikte aus der Welt zu r\u00e4umen sind . Gewaltverzicht und Deeskalation sind uns so sehr in Fleisch und Blut \u00fcbergegangen , dass wir uns schwer damit abfinden k\u00f6nnen , dass diese Prinzipien in einem einzigen Fall versagen : Fanatismus darf und kann man nicht mit Liberalit\u00e4t begegnen . Mit einem Selbstmord-Attent\u00e4ter kann man keinen Kompromiss schlie\u00dfen . Auch das Suchen nach m\u00f6glichen Motiven der T\u00e4ter hilft da nicht weiter . Sie beklagen den andauernden Konflikt zwischen Israelis und Pal\u00e4stinensern , dabei engagiert sich der Westen seit Jahren mit viel Geld und Krisendiplomatie f\u00fcr ein Ende der Gewalt . Sie beklagen die Armut der arabischen Welt , dabei stehen die autokratischen und nicht selten korrupten Eliten dort meist selbst jedem Aufschwung im Weg . Sie beklagen die Milit\u00e4rpr\u00e4senz der USA in Saudi-Arabien , dann h\u00e4tten sie die dortige K\u00f6nigsfamilie attackieren m\u00fcssen . Sie gei\u00dfeln den westlichen \" Kulturimperialismus \" und verkennen , dass sie sich entscheiden m\u00fcssen zwischen wirtschaftlichem"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "'token'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-59-e93b1a881784>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfail_adg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAnaphoraDocumentGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/arne/repos/pcc-annis-merged/maz176/anaphora/tosik/das/maz-3415.anaphora.das.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfail_adg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mfail_adg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mKeyError\u001b[0m: 'token'"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# different number of tokens ???"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}